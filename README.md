ğŸ“„ CV-Parsing-Pipeline
ğŸ§  Overview
CV-Parsing-Pipeline is an end-to-end automation pipeline that:

Converts CVs (PDFs) into plain text.

Extracts structured candidate information (skills, education, work experience, projects) using a language model.

Generates vector embeddings for these extracted fields.

Stores the vectorized data in a vector database (e.g. Qdrant) for fast, scalable search and retrieval.

This pipeline simplifies CV screening, enhances querying by semantic similarity, and is easy to integrate into existing recruitment tools.

âœ¨ Features
âœ… PDF to text conversion â€” extract raw text content with minimal dependencies.

âœ… LLM-powered parsing â€” structured extraction with a custom schema.

âœ… Field-specific vector embeddings â€” represent skills, experience, etc. numerically.

âœ… Efficient vector database operations â€” upsert and search vectors using Qdrant.

âœ… Error handling & logging â€” gracefully skip failed CVs.

âœ… Extensible and modular â€” easy to add new extractors or vector stores.

ğŸ§¬ Pipeline Structure
graphql
Copy
Edit
CV-Parsing-Pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_cvs/               # Input CV PDFs
â”‚   â”œâ”€â”€ processed_jsons/       # Output structured JSONs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pdf_to_text.py         # Converts PDFs to raw text
â”‚   â”œâ”€â”€ parsing_helper.py      # LLM-based information extraction
â”‚   â”œâ”€â”€ vector_embed.py        # Generates vector embeddings
â”‚   â”œâ”€â”€ vector_store.py        # Qdrant vector DB interactions
â”‚   â”œâ”€â”€ pipeline.py            # Orchestrates full process
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_pipeline.py       # Unit tests
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # Project overview
â””â”€â”€ main.py                    # CLI entry point
âš™ï¸ Installation
Clone the repository:

bash
Copy
Edit
git clone https://github.com/your_username/CV-Parsing-Pipeline.git
cd CV-Parsing-Pipeline
Create a virtual environment:

bash
Copy
Edit
python -m venv venv
source venv/bin/activate  # Linux/MacOS
venv\Scripts\activate     # Windows
Install dependencies:

bash
Copy
Edit
pip install -r requirements.txt
ğŸš€ Usage
Run the pipeline on a directory of CV PDFs:

bash
Copy
Edit
python main.py --input data/raw_cvs --output data/processed_jsons
Example CLI options:

lua
Copy
Edit
--input     Path to directory containing raw CV PDFs
--output    Output directory for extracted structured JSONs
--verbose   Enable detailed logs
ğŸ§  Parsing Details
The pipeline uses a language model (parsing_helper.py) to parse CVs into a strict JSON schema:

json
Copy
Edit
{
  "skills": ["Python", "NLP", "Machine Learning"],
  "work_experience": "Explicit work history as a string",
  "education": "Degree, major, years as a string",
  "projects": ["Project1 title or description"]
}
Missing fields return null or empty lists as appropriate.

ğŸ“Š Vector Storage
vector_embed.py encodes each field into a fixed-length vector.

vector_store.py handles Qdrant upserts and searches.

Weights for different fields can be tuned in get_user_requirements() or similar utility.

ğŸ§ª Tests
Unit tests are located in tests/. Run them with:

bash
Copy
Edit
pytest tests
ğŸ“œ License
This project is released under the MIT License â€” see LICENSE for details.

ğŸ’¬ Contributing
Contributions are welcome! Please open an issue or submit a pull request with improvements.

âœ¨ Happy parsing and vectorizing CVs! âœ¨
